{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuukienomoto/mahjong/blob/main/%E5%88%86%E9%A1%9E%E3%82%B3%E3%83%BC%E3%83%89%E3%82%B3%E3%83%94%E3%83%BCipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "数牌8万、字牌2万のアンダーサンプリング"
      ],
      "metadata": {
        "id": "USxP-KS9UjDb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "78880a77-1cb6-4244-d91b-e643f43e8cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid character '、' (U+3001) (ipython-input-768148582.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-768148582.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    数牌8万、字牌2万のアンダーサンプリング\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '、' (U+3001)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RblulqgXUin_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import zipfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import xml.etree.ElementTree as ET\n",
        "import optuna\n",
        "import re\n",
        "import gc # ガベージコレクション（メモリ解放用）\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# =============================================================================\n",
        "# 1. 環境設定・データの準備\n",
        "# =============================================================================\n",
        "print(\"\\n--- ステップ1: 環境設定とデータの準備 ---\")\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except Exception as e:\n",
        "    print(f\"Google Driveのマウントに失敗しました: {e}\")\n",
        "\n",
        "# ZIPファイルのパス\n",
        "ZIP_PATH = '/content/drive/MyDrive/dataset_2022-2024_houou_80k.zip'\n",
        "\n",
        "# 作業用ディレクトリ\n",
        "LOCAL_WORK_DIR = '/content/temp_xml_data/'      # XML解凍先\n",
        "BATCH_SAVE_DIR = '/content/temp_batches/'       # ★一時バッチ保存先\n",
        "\n",
        "# 最終保存先\n",
        "DATASET_PKL = '/content/drive/MyDrive/mahjong_balanced_dataset_200k.pkl'\n",
        "\n",
        "# ディレクトリ初期化\n",
        "if os.path.exists(BATCH_SAVE_DIR):\n",
        "    shutil.rmtree(BATCH_SAVE_DIR)\n",
        "os.makedirs(BATCH_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# ZIP解凍\n",
        "if not os.path.exists(LOCAL_WORK_DIR):\n",
        "    print(f\"★ZIPファイル: {ZIP_PATH}\")\n",
        "    if os.path.exists(ZIP_PATH):\n",
        "        print(\"ローカル環境に高速解凍中... (数分お待ちください)\")\n",
        "        try:\n",
        "            with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
        "                zip_ref.extractall(LOCAL_WORK_DIR)\n",
        "            print(\"解凍完了！\")\n",
        "        except Exception as e:\n",
        "            print(f\"解凍エラー: {e}\")\n",
        "            raise\n",
        "    else:\n",
        "        raise FileNotFoundError(\"ZIP file not found\")\n",
        "else:\n",
        "    print(\"既に解凍済みデータがあるため、それを使用します。\")\n",
        "\n",
        "# 解析対象フォルダ\n",
        "LOG_DIR = LOCAL_WORK_DIR\n",
        "print(f\"解析対象フォルダ: {LOG_DIR}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 2. ヘルパー関数 (ロジック)\n",
        "# =============================================================================\n",
        "# (ロジック部分は変更なし)\n",
        "\n",
        "def _parse_tile_type(tile_type_code):\n",
        "    if 0 <= tile_type_code <= 8:      return 0, tile_type_code + 1\n",
        "    elif 9 <= tile_type_code <= 17: return 1, (tile_type_code - 9) + 1\n",
        "    elif 18 <= tile_type_code <= 26: return 2, (tile_type_code - 18) + 1\n",
        "    elif 27 <= tile_type_code <= 33: return 3, (tile_type_code - 27) + 1\n",
        "    else: return -1, -1\n",
        "\n",
        "def count_dora(tile_code, dora_indicators):\n",
        "    dora_count = 0\n",
        "    t_type, t_num = _parse_tile_type(tile_code)\n",
        "    for ind in dora_indicators:\n",
        "        ind_type, ind_num = _parse_tile_type(ind)\n",
        "        if t_type == ind_type:\n",
        "            if t_type == 3:\n",
        "                target_num = (ind_num % 4) + 1\n",
        "                if ind_num >= 5: target_num = 5 + ((ind_num - 5 + 1) % 3)\n",
        "                if t_num == target_num: dora_count += 1\n",
        "            else:\n",
        "                target_num = 1 if ind_num == 9 else ind_num + 1\n",
        "                if t_num == target_num: dora_count += 1\n",
        "    return dora_count\n",
        "\n",
        "def parse_called_tile(m_int):\n",
        "    extracted = (m_int >> 8) & 0xFF\n",
        "    return extracted * 4\n",
        "\n",
        "def check_suji_fair(target_type, target_num, opponent_discards):\n",
        "    if target_type == 3: return 0\n",
        "    opp_discards_nums = []\n",
        "    for d in opponent_discards:\n",
        "        dt, dn = _parse_tile_type(d)\n",
        "        if dt == target_type:\n",
        "            opp_discards_nums.append(dn)\n",
        "\n",
        "    suji_map = {\n",
        "        1: [4], 2: [5], 3: [6],\n",
        "        4: [1, 7], 5: [2, 8], 6: [3, 9],\n",
        "        7: [4], 8: [5], 9: [6]\n",
        "    }\n",
        "    needed = suji_map.get(target_num, [])\n",
        "    if not needed: return 0\n",
        "    return 1 if all(n in opp_discards_nums for n in needed) else 0\n",
        "\n",
        "def get_kabe_fair(target_type, target_num, all_public_tiles):\n",
        "    if target_type == 3: return 0\n",
        "    check_nums = []\n",
        "    if target_num == 1: check_nums = [2]\n",
        "    elif target_num == 9: check_nums = [8]\n",
        "    elif target_num == 2: check_nums = [3]\n",
        "    elif target_num == 8: check_nums = [7]\n",
        "    elif target_num == 3: check_nums = [4]\n",
        "    elif target_num == 7: check_nums = [6]\n",
        "    elif target_num == 4: check_nums = [3, 5]\n",
        "    elif target_num == 5: check_nums = [4, 6]\n",
        "    elif target_num == 6: check_nums = [5, 7]\n",
        "\n",
        "    max_visible = 0\n",
        "    for n in check_nums:\n",
        "        check_id = (target_type * 9) + (n - 1)\n",
        "        count = all_public_tiles.count(check_id)\n",
        "        if count > max_visible: max_visible = count\n",
        "    return max_visible\n",
        "\n",
        "def get_my_yakuman_potential(my_hand_tiles):\n",
        "    tiles = [_parse_tile_type(t // 4) for t in my_hand_tiles]\n",
        "    yaochu_ids = set()\n",
        "    sangen_count = 0\n",
        "    jihai_count = 0\n",
        "    routou_count = 0\n",
        "    ryuuiiso_count = 0\n",
        "\n",
        "    for t_type, t_num in tiles:\n",
        "        if t_type == 3 or t_num == 1 or t_num == 9:\n",
        "            yaochu_ids.add(t_type * 10 + t_num)\n",
        "        if t_type == 3 and t_num >= 5:\n",
        "            sangen_count += 1\n",
        "        if t_type == 3:\n",
        "            jihai_count += 1\n",
        "        if t_type < 3 and (t_num == 1 or t_num == 9):\n",
        "            routou_count += 1\n",
        "        if (t_type == 2 and t_num in [2,3,4,6,8]) or (t_type == 3 and t_num == 6):\n",
        "            ryuuiiso_count += 1\n",
        "\n",
        "    scores = [\n",
        "        len(yaochu_ids), sangen_count * 2.5, jihai_count, routou_count, ryuuiiso_count\n",
        "    ]\n",
        "    return max(scores)\n",
        "\n",
        "def get_meld_threat_level(opp_idx, game_state, dora_indicators):\n",
        "    melds = game_state['open_hands'][opp_idx]\n",
        "    if not melds: return 0\n",
        "\n",
        "    threat = 0\n",
        "    colors = {0:0, 1:0, 2:0, 3:0}\n",
        "    dora_pon_count = 0\n",
        "\n",
        "    for meld in melds:\n",
        "        if not meld: continue\n",
        "        first = meld[0]\n",
        "        tt, tn = _parse_tile_type(first)\n",
        "\n",
        "        if tt != -1: colors[tt] += 1\n",
        "        d_val = count_dora(first // 4, dora_indicators)\n",
        "        if d_val > 0:\n",
        "            dora_pon_count += d_val\n",
        "        if tt == 3 and tn >= 5: threat += 15\n",
        "\n",
        "    threat += (dora_pon_count * 20)\n",
        "\n",
        "    for c in range(3):\n",
        "        if colors[c] + colors[3] >= 3 and colors[c] >= 2:\n",
        "            threat += 20\n",
        "            if colors[3] == 0: threat += 30\n",
        "\n",
        "    sangen = 0\n",
        "    for meld in melds:\n",
        "        first = meld[0] // 4\n",
        "        tt, tn = _parse_tile_type(first)\n",
        "        if tt == 3 and tn >= 5: sangen += 1\n",
        "    if sangen >= 2: threat += 80\n",
        "\n",
        "    return threat\n",
        "\n",
        "def calculate_features_fair(game_state, player_index, physical_code, is_tsumogiri, oya_player):\n",
        "    tile_type_code = physical_code // 4\n",
        "    t_type, t_num = _parse_tile_type(tile_type_code)\n",
        "    features = []\n",
        "\n",
        "    # 公開情報\n",
        "    public_tiles = []\n",
        "    public_tiles.extend(game_state['hands'][player_index])\n",
        "    for i in range(4):\n",
        "        public_tiles.extend(game_state['discards'][i])\n",
        "        for meld in game_state['open_hands'][i]:\n",
        "            public_tiles.extend(meld)\n",
        "\n",
        "    features.append(t_type)\n",
        "    features.append(t_num)\n",
        "    features.append(public_tiles.count(tile_type_code))\n",
        "\n",
        "    dora_val = 0\n",
        "    dora_indicators = game_state['dora_indicators']\n",
        "    dora_val += count_dora(tile_type_code, dora_indicators)\n",
        "    if physical_code in [16, 52, 88]: dora_val += 1\n",
        "    features.append(dora_val)\n",
        "\n",
        "    features.append(len(game_state['history']) // 4)\n",
        "    features.append(1 if is_tsumogiri else 0)\n",
        "\n",
        "    other_reach_indices = [i for i in range(4) if i != player_index and game_state['reach'][i]]\n",
        "    features.append(len(other_reach_indices))\n",
        "\n",
        "    risk_score = 0\n",
        "    if len(other_reach_indices) > 0:\n",
        "        for r_idx in other_reach_indices:\n",
        "            if tile_type_code in game_state['discards'][r_idx]: continue\n",
        "            this_risk = 0\n",
        "            if check_suji_fair(t_type, t_num, game_state['discards'][r_idx]) == 0:\n",
        "                this_risk += 50\n",
        "                if 4 <= t_num <= 6: this_risk += 20\n",
        "                kabe = get_kabe_fair(t_type, t_num, public_tiles)\n",
        "                if kabe < 1: this_risk += 30\n",
        "            else:\n",
        "                this_risk += 15\n",
        "            if this_risk > risk_score: risk_score = this_risk\n",
        "    features.append(risk_score)\n",
        "\n",
        "    live_degree = 0\n",
        "    visible_n = public_tiles.count(tile_type_code)\n",
        "    if visible_n == 0:\n",
        "        live_degree = 2\n",
        "        if t_type == 3: live_degree = 3\n",
        "    elif visible_n == 1:\n",
        "        live_degree = 1\n",
        "    features.append(live_degree)\n",
        "\n",
        "    features.append(get_kabe_fair(t_type, t_num, public_tiles))\n",
        "\n",
        "    max_meld_threat = 0\n",
        "    for i in range(4):\n",
        "        if i != player_index:\n",
        "            th = get_meld_threat_level(i, game_state, dora_indicators)\n",
        "            if th > max_meld_threat: max_meld_threat = th\n",
        "    features.append(max_meld_threat)\n",
        "\n",
        "    my_yakuman = get_my_yakuman_potential(game_state['hands'][player_index])\n",
        "    features.append(my_yakuman)\n",
        "\n",
        "    return features\n",
        "\n",
        "feature_names = [\n",
        "    '1.種類', '2.数字', '3.見え枚', '4.ドラ数',\n",
        "    '5.巡目', '6.ツモ切', '7.他家立直数',\n",
        "    '8.対リーチ危険度', '9.生牌度', '10.カベ強度',\n",
        "    '11.敵副露脅威度', '12.自手牌価値'\n",
        "]\n",
        "\n",
        "# =============================================================================\n",
        "# 4. XML解析 (バッチ保存方式でメモリクラッシュ回避)\n",
        "# =============================================================================\n",
        "\n",
        "def get_clean_tag(tag_str):\n",
        "    if '}' in tag_str:\n",
        "        return tag_str.split('}', 1)[1]\n",
        "    return tag_str\n",
        "\n",
        "print(\"\\n★ XML解析開始...\")\n",
        "xml_files = glob.glob(os.path.join(LOG_DIR, '**/*.xml'), recursive=True)\n",
        "print(f\"発見ファイル数: {len(xml_files)}\")\n",
        "\n",
        "if len(xml_files) == 0:\n",
        "    print(\"エラー: XMLファイルが見つかりません。\")\n",
        "else:\n",
        "    # バッチ処理用の変数\n",
        "    current_batch_data = []\n",
        "    BATCH_SIZE_FILES = 2000 # 2000ファイルごとに保存してメモリ解放\n",
        "    batch_count = 0\n",
        "\n",
        "    draw_ptn = re.compile(r'^[TUVW]\\d+$')\n",
        "    discard_ptn = re.compile(r'^[DEFG]\\d+$')\n",
        "\n",
        "    count_suuhai_pos = 0\n",
        "    count_jihai_pos = 0\n",
        "\n",
        "    TARGET_SUUHAI = 80000\n",
        "    TARGET_JIHAI = 20000\n",
        "\n",
        "    print(f\"目標: 数牌正例 {TARGET_SUUHAI}件, 字牌正例 {TARGET_JIHAI}件\")\n",
        "    print(\"★メモリ保護のため、小分けにして保存(Batch processing)を行います。\")\n",
        "\n",
        "    for i_file, file_path in enumerate(xml_files):\n",
        "\n",
        "        # --- バッチ保存処理 ---\n",
        "        if i_file > 0 and i_file % BATCH_SIZE_FILES == 0:\n",
        "            # ここまで溜まったデータを保存\n",
        "            if current_batch_data:\n",
        "                batch_df = pd.DataFrame(current_batch_data)\n",
        "                save_path = os.path.join(BATCH_SAVE_DIR, f'batch_{batch_count}.pkl')\n",
        "                batch_df.to_pickle(save_path)\n",
        "                batch_count += 1\n",
        "\n",
        "                # メモリ解放\n",
        "                del batch_df, current_batch_data\n",
        "                current_batch_data = []\n",
        "                gc.collect() # 強制メモリ解放\n",
        "\n",
        "                print(f\" [Batch] {i_file}ファイル処理完了 -> batch_{batch_count-1}.pkl 保存 (現在: 数牌正{count_suuhai_pos}/字牌正{count_jihai_pos})\")\n",
        "\n",
        "            # 終了判定\n",
        "            if count_suuhai_pos >= TARGET_SUUHAI and count_jihai_pos >= TARGET_JIHAI:\n",
        "                print(\"★目標のデータ数に達しました。解析ループを終了します。\")\n",
        "                break\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                try:\n",
        "                    tree = ET.parse(f)\n",
        "                    root = tree.getroot()\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            game_state = {\n",
        "                'hands': [[],[],[],[]], 'discards': [[],[],[],[]], 'open_hands': [[],[],[],[]],\n",
        "                'reach': [False]*4, 'history': [], 'dora_indicators': []\n",
        "            }\n",
        "            last_draw = [-1]*4\n",
        "            last_discard_event = None\n",
        "            oya_player = 0\n",
        "\n",
        "            for tag in root:\n",
        "                tag_name = get_clean_tag(tag.tag)\n",
        "\n",
        "                if tag_name == 'INIT':\n",
        "                    if last_discard_event:\n",
        "                        current_batch_data.append(last_discard_event)\n",
        "                        if last_discard_event['y'] == 1:\n",
        "                            if last_discard_event['X'][0] == 3: count_jihai_pos += 1\n",
        "                            else: count_suuhai_pos += 1\n",
        "\n",
        "                    oya_player = int(tag.attrib.get('oya', 0))\n",
        "                    seed = tag.attrib.get('seed', '0,0,0,0,0,0').split(',')\n",
        "                    dora_ind = int(seed[5]) // 4 if len(seed) > 5 else 0\n",
        "\n",
        "                    game_state = {\n",
        "                        'hands': [[],[],[],[]], 'discards': [[],[],[],[]], 'open_hands': [[],[],[],[]],\n",
        "                        'reach': [False]*4, 'history': [], 'dora_indicators': [dora_ind]\n",
        "                    }\n",
        "                    last_discard_event = None\n",
        "                    for p in range(4):\n",
        "                        h = tag.attrib.get(f'hai{p}')\n",
        "                        if h: game_state['hands'][p] = [int(s)//4 for s in h.split(',')]\n",
        "\n",
        "                if draw_ptn.match(tag_name):\n",
        "                    p = {'T':0,'U':1,'V':2,'W':3}[tag_name[0]]\n",
        "                    phys = int(tag_name[1:])\n",
        "                    last_draw[p] = phys\n",
        "                    game_state['hands'][p].append(phys // 4)\n",
        "\n",
        "                if discard_ptn.match(tag_name):\n",
        "                    p = {'D':0,'E':1,'F':2,'G':3}[tag_name[0]]\n",
        "                    phys = int(tag_name[1:])\n",
        "\n",
        "                    if last_discard_event:\n",
        "                        current_batch_data.append(last_discard_event)\n",
        "                        if last_discard_event['y'] == 1:\n",
        "                            if last_discard_event['X'][0] == 3: count_jihai_pos += 1\n",
        "                            else: count_suuhai_pos += 1\n",
        "\n",
        "                    is_tsumogiri = (phys == last_draw[p])\n",
        "                    feats = calculate_features_fair(game_state, p, phys, is_tsumogiri, oya_player)\n",
        "\n",
        "                    last_discard_event = {'X': feats, 'y': 0}\n",
        "\n",
        "                    t_type = phys // 4\n",
        "                    if t_type in game_state['hands'][p]: game_state['hands'][p].remove(t_type)\n",
        "                    game_state['discards'][p].append(t_type)\n",
        "                    game_state['history'].append((p, t_type))\n",
        "\n",
        "                if tag_name == 'N':\n",
        "                    who = int(tag.attrib.get('who'))\n",
        "                    m = int(tag.attrib.get('m'))\n",
        "                    called_tile_id = parse_called_tile(m)\n",
        "                    c_type = called_tile_id // 4\n",
        "                    removed = 0\n",
        "                    temp = game_state['hands'][who][:]\n",
        "                    for h in temp:\n",
        "                        if h == c_type and removed < 2:\n",
        "                            game_state['hands'][who].remove(h)\n",
        "                            removed += 1\n",
        "                    game_state['open_hands'][who].append([called_tile_id]*3)\n",
        "\n",
        "                    if last_discard_event:\n",
        "                        current_batch_data.append(last_discard_event)\n",
        "                        if last_discard_event['y'] == 1:\n",
        "                            if last_discard_event['X'][0] == 3: count_jihai_pos += 1\n",
        "                            else: count_suuhai_pos += 1\n",
        "                        last_discard_event = None\n",
        "\n",
        "                if tag_name == 'REACH' and tag.attrib.get('step') == '1':\n",
        "                    who_reach = int(tag.attrib.get('who'))\n",
        "                    game_state['reach'][who_reach] = True\n",
        "\n",
        "                if tag_name == 'DORA':\n",
        "                    hai = int(tag.attrib.get('hai'))\n",
        "                    game_state['dora_indicators'].append(hai // 4)\n",
        "\n",
        "                if tag_name == 'AGARI':\n",
        "                    loser = tag.attrib.get('fromWho')\n",
        "                    if loser and int(tag.attrib.get('who')) != int(loser):\n",
        "                        if last_discard_event:\n",
        "                            last_discard_event['y'] = 1\n",
        "                            current_batch_data.append(last_discard_event)\n",
        "                            if last_discard_event['X'][0] == 3: count_jihai_pos += 1\n",
        "                            else: count_suuhai_pos += 1\n",
        "                            last_discard_event = None\n",
        "\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # ループ終了後の残りを保存\n",
        "    if current_batch_data:\n",
        "        batch_df = pd.DataFrame(current_batch_data)\n",
        "        save_path = os.path.join(BATCH_SAVE_DIR, f'batch_{batch_count}.pkl')\n",
        "        batch_df.to_pickle(save_path)\n",
        "        print(f\" [Batch] 最終バッチ保存完了 -> batch_{batch_count}.pkl\")\n",
        "        del batch_df, current_batch_data\n",
        "        gc.collect()\n",
        "\n",
        "# =============================================================================\n",
        "# 5. ★アンダーサンプリング (バッチファイルを統合しながら抽出)\n",
        "# =============================================================================\n",
        "print(\"\\n--- ステップ2: 保存されたバッチファイルからサンプリング統合 ---\")\n",
        "\n",
        "batch_files = glob.glob(os.path.join(BATCH_SAVE_DIR, '*.pkl'))\n",
        "if len(batch_files) == 0:\n",
        "    print(\"エラー: バッチファイルが生成されていません。\")\n",
        "else:\n",
        "    # データを溜めるリスト\n",
        "    all_suuhai_pos = []\n",
        "    all_suuhai_neg = []\n",
        "    all_jihai_pos = []\n",
        "    all_jihai_neg = []\n",
        "\n",
        "    # 目標数\n",
        "    TARGET_SUUHAI_EACH = 80000\n",
        "    TARGET_JIHAI_EACH = 20000\n",
        "\n",
        "    print(f\"全{len(batch_files)}個のバッチファイルを処理中...\")\n",
        "\n",
        "    for b_file in batch_files:\n",
        "        try:\n",
        "            df_b = pd.read_pickle(b_file)\n",
        "            if len(df_b) == 0: continue\n",
        "\n",
        "            # 特徴量展開\n",
        "            X_temp = pd.DataFrame(df_b['X'].tolist(), columns=feature_names)\n",
        "            df_full = pd.concat([df_b['y'], X_temp], axis=1)\n",
        "            df_full['is_jihai'] = df_full['1.種類'] == 3\n",
        "\n",
        "            # カテゴリ分けして一時リストに追加\n",
        "            s_p = df_full[(df_full['is_jihai'] == False) & (df_full['y'] == 1)]\n",
        "            s_n = df_full[(df_full['is_jihai'] == False) & (df_full['y'] == 0)]\n",
        "            j_p = df_full[(df_full['is_jihai'] == True) & (df_full['y'] == 1)]\n",
        "            j_n = df_full[(df_full['is_jihai'] == True) & (df_full['y'] == 0)]\n",
        "\n",
        "            # 必要な分だけ保持（メモリ節約のため、負例はランダムに少しだけ取る）\n",
        "            # 正例は全て確保\n",
        "            all_suuhai_pos.append(s_p)\n",
        "            all_jihai_pos.append(j_p)\n",
        "\n",
        "            # 負例はバッチごとに少しサンプリングして確保（全部持つと死ぬので）\n",
        "            # バッチ数で割った数より少し多めに確保しておく\n",
        "            sample_rate = 0.2 # 負例は20%くらい確保しておけば十分\n",
        "            if len(s_n) > 0: all_suuhai_neg.append(s_n.sample(frac=sample_rate))\n",
        "            if len(j_n) > 0: all_jihai_neg.append(j_n.sample(frac=sample_rate))\n",
        "\n",
        "            del df_b, df_full, s_p, s_n, j_p, j_n, X_temp\n",
        "            gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Skip file {b_file}: {e}\")\n",
        "\n",
        "    # 全結合\n",
        "    print(\"データを結合中...\")\n",
        "    df_suuhai_pos = pd.concat(all_suuhai_pos)\n",
        "    df_suuhai_neg = pd.concat(all_suuhai_neg)\n",
        "    df_jihai_pos = pd.concat(all_jihai_pos)\n",
        "    df_jihai_neg = pd.concat(all_jihai_neg)\n",
        "\n",
        "    print(f\"集計結果(正例): 数牌 {len(df_suuhai_pos)}, 字牌 {len(df_jihai_pos)}\")\n",
        "\n",
        "    # 最終サンプリング\n",
        "    n_sp = min(len(df_suuhai_pos), TARGET_SUUHAI_EACH)\n",
        "    n_sn = TARGET_SUUHAI_EACH\n",
        "\n",
        "    n_jp = min(len(df_jihai_pos), TARGET_JIHAI_EACH)\n",
        "    n_jn = TARGET_JIHAI_EACH\n",
        "\n",
        "    # 負例が足りない場合\n",
        "    if len(df_suuhai_neg) < n_sn: n_sn = len(df_suuhai_neg)\n",
        "    if len(df_jihai_neg) < n_jn: n_jn = len(df_jihai_neg)\n",
        "\n",
        "    final_df = pd.concat([\n",
        "        df_suuhai_pos.sample(n=n_sp, random_state=42),\n",
        "        df_suuhai_neg.sample(n=n_sn, random_state=42),\n",
        "        df_jihai_pos.sample(n=n_jp, random_state=42),\n",
        "        df_jihai_neg.sample(n=n_jn, random_state=42)\n",
        "    ]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    print(f\"\\n★最終データセット作成完了: {len(final_df)}件\")\n",
        "    print(final_df['y'].value_counts())\n",
        "\n",
        "    # 保存\n",
        "    final_df.to_pickle(DATASET_PKL)\n",
        "    print(f\"データセット保存完了: {DATASET_PKL}\")\n",
        "\n",
        "    # =============================================================================\n",
        "    # 6. Optuna & 学習\n",
        "    # =============================================================================\n",
        "    X = final_df[feature_names]\n",
        "    y = final_df['y']\n",
        "    cat_features = ['1.種類']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(\"\\n--- ステップ3: Optunaによるハイパーパラメータ探索 ---\")\n",
        "\n",
        "    def objective(trial):\n",
        "        param = {\n",
        "            'objective': 'binary',\n",
        "            'metric': 'binary_logloss',\n",
        "            'verbosity': -1,\n",
        "            'boosting_type': 'gbdt',\n",
        "            'n_estimators': 300,\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "            'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
        "            'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
        "            'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
        "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        }\n",
        "\n",
        "        model = lgb.LGBMClassifier(**param)\n",
        "        model.fit(X_train, y_train, categorical_feature=cat_features)\n",
        "        preds = model.predict(X_test)\n",
        "        return accuracy_score(y_test, preds)\n",
        "\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=15)\n",
        "\n",
        "    print(f\"★ Best Params: {study.best_params}\")\n",
        "\n",
        "    # --- 最終学習 ---\n",
        "    print(\"\\n--- 最終学習 ---\")\n",
        "    best_params = study.best_params\n",
        "    final_model = lgb.LGBMClassifier(\n",
        "        objective='binary',\n",
        "        metric='binary_logloss',\n",
        "        n_estimators=5000,\n",
        "        learning_rate=best_params.get('learning_rate', 0.05),\n",
        "        num_leaves=best_params.get('num_leaves', 31),\n",
        "        max_depth=best_params.get('max_depth', -1),\n",
        "        min_child_samples=best_params.get('min_child_samples', 20),\n",
        "        subsample=best_params.get('subsample', 0.8),\n",
        "        colsample_bytree=best_params.get('colsample_bytree', 0.8)\n",
        "    )\n",
        "\n",
        "    final_model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_test, y_test)],\n",
        "        eval_metric='auc',\n",
        "        callbacks=[lgb.early_stopping(100)],\n",
        "        categorical_feature=cat_features\n",
        "    )\n",
        "\n",
        "    # =============================================================================\n",
        "    # 7. 結果評価\n",
        "    # =============================================================================\n",
        "    print(\"\\n--- 評価結果 ---\")\n",
        "    preds = final_model.predict(X_test)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, preds):.4f}\")\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, preds))\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, preds))\n",
        "\n",
        "    print(\"\\n★ 特徴量重要度\")\n",
        "    imp = pd.DataFrame({'Feature': feature_names, 'Importance': final_model.feature_importances_}).sort_values('Importance', ascending=False)\n",
        "    print(imp.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6W08AXyIlxJ",
        "outputId": "9eff7d9b-2554-471d-c1e1-4e83efe44864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna) (6.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.45)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "\n",
            "--- ステップ1: 環境設定とデータの準備 ---\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "既に解凍済みデータがあるため、それを使用します。\n",
            "解析対象フォルダ: /content/temp_xml_data/\n",
            "\n",
            "★ XML解析開始...\n",
            "発見ファイル数: 80000\n",
            "目標: 数牌正例 80000件, 字牌正例 20000件\n",
            "★メモリ保護のため、小分けにして保存(Batch processing)を行います。\n",
            " [Batch] 2000ファイル処理完了 -> batch_0.pkl 保存 (現在: 数牌正9811/字牌正788)\n",
            " [Batch] 4000ファイル処理完了 -> batch_1.pkl 保存 (現在: 数牌正19473/字牌正1613)\n",
            " [Batch] 6000ファイル処理完了 -> batch_2.pkl 保存 (現在: 数牌正29190/字牌正2418)\n",
            " [Batch] 8000ファイル処理完了 -> batch_3.pkl 保存 (現在: 数牌正38978/字牌正3166)\n",
            " [Batch] 10000ファイル処理完了 -> batch_4.pkl 保存 (現在: 数牌正48756/字牌正4003)\n",
            " [Batch] 12000ファイル処理完了 -> batch_5.pkl 保存 (現在: 数牌正58519/字牌正4811)\n",
            " [Batch] 14000ファイル処理完了 -> batch_6.pkl 保存 (現在: 数牌正68267/字牌正5634)\n",
            " [Batch] 16000ファイル処理完了 -> batch_7.pkl 保存 (現在: 数牌正78001/字牌正6429)\n",
            " [Batch] 18000ファイル処理完了 -> batch_8.pkl 保存 (現在: 数牌正87887/字牌正7220)\n",
            " [Batch] 20000ファイル処理完了 -> batch_9.pkl 保存 (現在: 数牌正97671/字牌正8020)\n",
            " [Batch] 22000ファイル処理完了 -> batch_10.pkl 保存 (現在: 数牌正107306/字牌正8852)\n",
            " [Batch] 24000ファイル処理完了 -> batch_11.pkl 保存 (現在: 数牌正117182/字牌正9654)\n",
            " [Batch] 26000ファイル処理完了 -> batch_12.pkl 保存 (現在: 数牌正126987/字牌正10423)\n",
            " [Batch] 28000ファイル処理完了 -> batch_13.pkl 保存 (現在: 数牌正136618/字牌正11204)\n",
            " [Batch] 30000ファイル処理完了 -> batch_14.pkl 保存 (現在: 数牌正146266/字牌正12024)\n",
            " [Batch] 32000ファイル処理完了 -> batch_15.pkl 保存 (現在: 数牌正156016/字牌正12803)\n",
            " [Batch] 34000ファイル処理完了 -> batch_16.pkl 保存 (現在: 数牌正165694/字牌正13641)\n",
            " [Batch] 36000ファイル処理完了 -> batch_17.pkl 保存 (現在: 数牌正175442/字牌正14460)\n",
            " [Batch] 38000ファイル処理完了 -> batch_18.pkl 保存 (現在: 数牌正185217/字牌正15314)\n",
            " [Batch] 40000ファイル処理完了 -> batch_19.pkl 保存 (現在: 数牌正195006/字牌正16098)\n",
            " [Batch] 42000ファイル処理完了 -> batch_20.pkl 保存 (現在: 数牌正204662/字牌正16955)\n",
            " [Batch] 44000ファイル処理完了 -> batch_21.pkl 保存 (現在: 数牌正214396/字牌正17782)\n",
            " [Batch] 46000ファイル処理完了 -> batch_22.pkl 保存 (現在: 数牌正224043/字牌正18575)\n",
            " [Batch] 48000ファイル処理完了 -> batch_23.pkl 保存 (現在: 数牌正233681/字牌正19384)\n",
            " [Batch] 50000ファイル処理完了 -> batch_24.pkl 保存 (現在: 数牌正243252/字牌正20194)\n",
            "★目標のデータ数に達しました。解析ループを終了します。\n",
            "\n",
            "--- ステップ2: 保存されたバッチファイルからサンプリング統合 ---\n",
            "全25個のバッチファイルを処理中...\n",
            "データを結合中...\n",
            "集計結果(正例): 数牌 243252, 字牌 20194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-15 04:20:35,999] A new study created in memory with name: no-name-d4463478-f155-4e99-b2b1-200098d88cfb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "★最終データセット作成完了: 200000件\n",
            "y\n",
            "0    100000\n",
            "1    100000\n",
            "Name: count, dtype: int64\n",
            "データセット保存完了: /content/drive/MyDrive/mahjong_balanced_dataset_200k.pkl\n",
            "\n",
            "--- ステップ3: Optunaによるハイパーパラメータ探索 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-15 04:20:42,997] Trial 0 finished with value: 0.7866 and parameters: {'learning_rate': 0.058619631653518495, 'num_leaves': 21, 'max_depth': 5, 'min_child_samples': 42, 'subsample': 0.8237052602311032, 'colsample_bytree': 0.9587347855859911}. Best is trial 0 with value: 0.7866.\n",
            "[I 2026-01-15 04:20:51,818] Trial 1 finished with value: 0.778725 and parameters: {'learning_rate': 0.2775440966176624, 'num_leaves': 145, 'max_depth': 11, 'min_child_samples': 78, 'subsample': 0.7004963572751877, 'colsample_bytree': 0.6905591354046439}. Best is trial 0 with value: 0.7866.\n",
            "[I 2026-01-15 04:20:59,846] Trial 2 finished with value: 0.783725 and parameters: {'learning_rate': 0.20454861024915846, 'num_leaves': 89, 'max_depth': 15, 'min_child_samples': 82, 'subsample': 0.9502193067160039, 'colsample_bytree': 0.6633071128678614}. Best is trial 0 with value: 0.7866.\n",
            "[I 2026-01-15 04:21:08,543] Trial 3 finished with value: 0.787075 and parameters: {'learning_rate': 0.030223383778425407, 'num_leaves': 68, 'max_depth': 9, 'min_child_samples': 64, 'subsample': 0.5500331359017955, 'colsample_bytree': 0.9045797077481199}. Best is trial 3 with value: 0.787075.\n",
            "[I 2026-01-15 04:21:18,552] Trial 4 finished with value: 0.783475 and parameters: {'learning_rate': 0.2801466633924617, 'num_leaves': 129, 'max_depth': 7, 'min_child_samples': 73, 'subsample': 0.8418425770926634, 'colsample_bytree': 0.7366906557857545}. Best is trial 3 with value: 0.787075.\n",
            "[I 2026-01-15 04:21:30,078] Trial 5 finished with value: 0.786625 and parameters: {'learning_rate': 0.06842377216552956, 'num_leaves': 137, 'max_depth': 12, 'min_child_samples': 70, 'subsample': 0.8320996178480217, 'colsample_bytree': 0.5925582164008278}. Best is trial 3 with value: 0.787075.\n",
            "[I 2026-01-15 04:21:36,843] Trial 6 finished with value: 0.78575 and parameters: {'learning_rate': 0.22246041492903987, 'num_leaves': 56, 'max_depth': 6, 'min_child_samples': 86, 'subsample': 0.5037572964232522, 'colsample_bytree': 0.6680011348349921}. Best is trial 3 with value: 0.787075.\n",
            "[I 2026-01-15 04:21:45,875] Trial 7 finished with value: 0.784075 and parameters: {'learning_rate': 0.2595255389255355, 'num_leaves': 77, 'max_depth': 7, 'min_child_samples': 91, 'subsample': 0.7338917528511393, 'colsample_bytree': 0.6269197478597832}. Best is trial 3 with value: 0.787075.\n",
            "[I 2026-01-15 04:21:52,387] Trial 8 finished with value: 0.7864 and parameters: {'learning_rate': 0.15965765093834886, 'num_leaves': 86, 'max_depth': 5, 'min_child_samples': 49, 'subsample': 0.8142168025604255, 'colsample_bytree': 0.6131938227812398}. Best is trial 3 with value: 0.787075.\n",
            "[I 2026-01-15 04:22:02,607] Trial 9 finished with value: 0.78575 and parameters: {'learning_rate': 0.08763358607538606, 'num_leaves': 146, 'max_depth': 14, 'min_child_samples': 56, 'subsample': 0.9373261576994603, 'colsample_bytree': 0.6507761865259021}. Best is trial 3 with value: 0.787075.\n",
            "[I 2026-01-15 04:22:10,950] Trial 10 finished with value: 0.78655 and parameters: {'learning_rate': 0.014911287979832622, 'num_leaves': 42, 'max_depth': 9, 'min_child_samples': 19, 'subsample': 0.5090687531557774, 'colsample_bytree': 0.8985279873824983}. Best is trial 3 with value: 0.787075.\n",
            "[I 2026-01-15 04:22:20,216] Trial 11 finished with value: 0.786475 and parameters: {'learning_rate': 0.09313813098658286, 'num_leaves': 102, 'max_depth': 12, 'min_child_samples': 65, 'subsample': 0.6467802067923575, 'colsample_bytree': 0.5319530288842678}. Best is trial 3 with value: 0.787075.\n",
            "[I 2026-01-15 04:22:31,568] Trial 12 finished with value: 0.786875 and parameters: {'learning_rate': 0.012734210943432273, 'num_leaves': 115, 'max_depth': 9, 'min_child_samples': 37, 'subsample': 0.6152719050142389, 'colsample_bytree': 0.8331335883769924}. Best is trial 3 with value: 0.787075.\n",
            "[I 2026-01-15 04:22:42,057] Trial 13 finished with value: 0.786875 and parameters: {'learning_rate': 0.012176646479566209, 'num_leaves': 113, 'max_depth': 9, 'min_child_samples': 30, 'subsample': 0.6049278274163395, 'colsample_bytree': 0.8437291621756424}. Best is trial 3 with value: 0.787075.\n",
            "[I 2026-01-15 04:22:48,684] Trial 14 finished with value: 0.787025 and parameters: {'learning_rate': 0.12887571721547048, 'num_leaves': 65, 'max_depth': 9, 'min_child_samples': 38, 'subsample': 0.5859276065350666, 'colsample_bytree': 0.8323794816314946}. Best is trial 3 with value: 0.787075.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "★ Best Params: {'learning_rate': 0.030223383778425407, 'num_leaves': 68, 'max_depth': 9, 'min_child_samples': 64, 'subsample': 0.5500331359017955, 'colsample_bytree': 0.9045797077481199}\n",
            "\n",
            "--- 最終学習 ---\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[260]\tvalid_0's auc: 0.870648\tvalid_0's binary_logloss: 0.443141\n",
            "\n",
            "--- 評価結果 ---\n",
            "Accuracy: 0.7871\n",
            "\n",
            "Confusion Matrix:\n",
            "[[15081  4813]\n",
            " [ 3704 16402]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.76      0.78     19894\n",
            "           1       0.77      0.82      0.79     20106\n",
            "\n",
            "    accuracy                           0.79     40000\n",
            "   macro avg       0.79      0.79      0.79     40000\n",
            "weighted avg       0.79      0.79      0.79     40000\n",
            "\n",
            "\n",
            "★ 特徴量重要度\n",
            "  Feature  Importance\n",
            "     5.巡目        3223\n",
            "     2.数字        3101\n",
            "    3.見え枚        2068\n",
            " 12.自手牌価値        1885\n",
            "  10.カベ強度        1782\n",
            "8.対リーチ危険度        1364\n",
            "     1.種類        1280\n",
            "  7.他家立直数         938\n",
            "    6.ツモ切         813\n",
            "11.敵副露脅威度         376\n",
            "    4.ドラ数         349\n",
            "    9.生牌度         116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "数牌20万、字牌2万のアンダーサンプリング"
      ],
      "metadata": {
        "id": "VqUyzyUcUpFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import zipfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import xml.etree.ElementTree as ET\n",
        "import optuna\n",
        "import re\n",
        "import gc\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# =============================================================================\n",
        "# 1. 環境設定・データの準備\n",
        "# =============================================================================\n",
        "print(\"\\n--- ステップ1: 環境設定とデータの準備 ---\")\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except Exception as e:\n",
        "    print(f\"Google Driveのマウントに失敗しました: {e}\")\n",
        "\n",
        "# ZIPファイルのパス\n",
        "ZIP_PATH = '/content/drive/MyDrive/dataset_2022-2024_houou_80k.zip'\n",
        "\n",
        "# 作業用ディレクトリ\n",
        "LOCAL_WORK_DIR = '/content/temp_xml_data/'\n",
        "BATCH_SAVE_DIR = '/content/temp_batches/'\n",
        "\n",
        "# ★保存名変更: 440k (数牌20万ペア + 字牌2万ペア)\n",
        "DATASET_PKL = '/content/drive/MyDrive/mahjong_balanced_dataset_440k.pkl'\n",
        "\n",
        "# バッチフォルダの初期化\n",
        "if os.path.exists(BATCH_SAVE_DIR):\n",
        "    shutil.rmtree(BATCH_SAVE_DIR)\n",
        "os.makedirs(BATCH_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# ZIP解凍\n",
        "if not os.path.exists(LOCAL_WORK_DIR):\n",
        "    print(f\"★ZIPファイル: {ZIP_PATH}\")\n",
        "    if os.path.exists(ZIP_PATH):\n",
        "        print(\"ローカル環境に高速解凍中... (数分お待ちください)\")\n",
        "        try:\n",
        "            with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
        "                zip_ref.extractall(LOCAL_WORK_DIR)\n",
        "            print(\"解凍完了！\")\n",
        "        except Exception as e:\n",
        "            print(f\"解凍エラー: {e}\")\n",
        "            raise\n",
        "    else:\n",
        "        raise FileNotFoundError(\"ZIP file not found\")\n",
        "else:\n",
        "    print(\"既に解凍済みデータがあるため、それを使用します。\")\n",
        "\n",
        "LOG_DIR = LOCAL_WORK_DIR\n",
        "\n",
        "# =============================================================================\n",
        "# 2. ヘルパー関数 (ロジック)\n",
        "# =============================================================================\n",
        "def _parse_tile_type(tile_type_code):\n",
        "    if 0 <= tile_type_code <= 8:      return 0, tile_type_code + 1\n",
        "    elif 9 <= tile_type_code <= 17: return 1, (tile_type_code - 9) + 1\n",
        "    elif 18 <= tile_type_code <= 26: return 2, (tile_type_code - 18) + 1\n",
        "    elif 27 <= tile_type_code <= 33: return 3, (tile_type_code - 27) + 1\n",
        "    else: return -1, -1\n",
        "\n",
        "def count_dora(tile_code, dora_indicators):\n",
        "    dora_count = 0\n",
        "    t_type, t_num = _parse_tile_type(tile_code)\n",
        "    for ind in dora_indicators:\n",
        "        ind_type, ind_num = _parse_tile_type(ind)\n",
        "        if t_type == ind_type:\n",
        "            if t_type == 3:\n",
        "                target_num = (ind_num % 4) + 1\n",
        "                if ind_num >= 5: target_num = 5 + ((ind_num - 5 + 1) % 3)\n",
        "                if t_num == target_num: dora_count += 1\n",
        "            else:\n",
        "                target_num = 1 if ind_num == 9 else ind_num + 1\n",
        "                if t_num == target_num: dora_count += 1\n",
        "    return dora_count\n",
        "\n",
        "def parse_called_tile(m_int):\n",
        "    extracted = (m_int >> 8) & 0xFF\n",
        "    return extracted * 4\n",
        "\n",
        "def check_suji_fair(target_type, target_num, opponent_discards):\n",
        "    if target_type == 3: return 0\n",
        "    opp_discards_nums = []\n",
        "    for d in opponent_discards:\n",
        "        dt, dn = _parse_tile_type(d)\n",
        "        if dt == target_type:\n",
        "            opp_discards_nums.append(dn)\n",
        "\n",
        "    suji_map = {\n",
        "        1: [4], 2: [5], 3: [6],\n",
        "        4: [1, 7], 5: [2, 8], 6: [3, 9],\n",
        "        7: [4], 8: [5], 9: [6]\n",
        "    }\n",
        "    needed = suji_map.get(target_num, [])\n",
        "    if not needed: return 0\n",
        "    return 1 if all(n in opp_discards_nums for n in needed) else 0\n",
        "\n",
        "def get_kabe_fair(target_type, target_num, all_public_tiles):\n",
        "    if target_type == 3: return 0\n",
        "    check_nums = []\n",
        "    if target_num == 1: check_nums = [2]\n",
        "    elif target_num == 9: check_nums = [8]\n",
        "    elif target_num == 2: check_nums = [3]\n",
        "    elif target_num == 8: check_nums = [7]\n",
        "    elif target_num == 3: check_nums = [4]\n",
        "    elif target_num == 7: check_nums = [6]\n",
        "    elif target_num == 4: check_nums = [3, 5]\n",
        "    elif target_num == 5: check_nums = [4, 6]\n",
        "    elif target_num == 6: check_nums = [5, 7]\n",
        "\n",
        "    max_visible = 0\n",
        "    for n in check_nums:\n",
        "        check_id = (target_type * 9) + (n - 1)\n",
        "        count = all_public_tiles.count(check_id)\n",
        "        if count > max_visible: max_visible = count\n",
        "    return max_visible\n",
        "\n",
        "def get_my_yakuman_potential(my_hand_tiles):\n",
        "    tiles = [_parse_tile_type(t // 4) for t in my_hand_tiles]\n",
        "    yaochu_ids = set()\n",
        "    sangen_count = 0\n",
        "    jihai_count = 0\n",
        "    routou_count = 0\n",
        "    ryuuiiso_count = 0\n",
        "\n",
        "    for t_type, t_num in tiles:\n",
        "        if t_type == 3 or t_num == 1 or t_num == 9:\n",
        "            yaochu_ids.add(t_type * 10 + t_num)\n",
        "        if t_type == 3 and t_num >= 5:\n",
        "            sangen_count += 1\n",
        "        if t_type == 3:\n",
        "            jihai_count += 1\n",
        "        if t_type < 3 and (t_num == 1 or t_num == 9):\n",
        "            routou_count += 1\n",
        "        if (t_type == 2 and t_num in [2,3,4,6,8]) or (t_type == 3 and t_num == 6):\n",
        "            ryuuiiso_count += 1\n",
        "\n",
        "    scores = [\n",
        "        len(yaochu_ids), sangen_count * 2.5, jihai_count, routou_count, ryuuiiso_count\n",
        "    ]\n",
        "    return max(scores)\n",
        "\n",
        "def get_meld_threat_level(opp_idx, game_state, dora_indicators):\n",
        "    melds = game_state['open_hands'][opp_idx]\n",
        "    if not melds: return 0\n",
        "\n",
        "    threat = 0\n",
        "    colors = {0:0, 1:0, 2:0, 3:0}\n",
        "    dora_pon_count = 0\n",
        "\n",
        "    for meld in melds:\n",
        "        if not meld: continue\n",
        "        first = meld[0]\n",
        "        tt, tn = _parse_tile_type(first)\n",
        "\n",
        "        if tt != -1: colors[tt] += 1\n",
        "        d_val = count_dora(first // 4, dora_indicators)\n",
        "        if d_val > 0:\n",
        "            dora_pon_count += d_val\n",
        "        if tt == 3 and tn >= 5: threat += 15\n",
        "\n",
        "    threat += (dora_pon_count * 20)\n",
        "\n",
        "    for c in range(3):\n",
        "        if colors[c] + colors[3] >= 3 and colors[c] >= 2:\n",
        "            threat += 20\n",
        "            if colors[3] == 0: threat += 30\n",
        "\n",
        "    sangen = 0\n",
        "    for meld in melds:\n",
        "        first = meld[0] // 4\n",
        "        tt, tn = _parse_tile_type(first)\n",
        "        if tt == 3 and tn >= 5: sangen += 1\n",
        "    if sangen >= 2: threat += 80\n",
        "\n",
        "    return threat\n",
        "\n",
        "def calculate_features_fair(game_state, player_index, physical_code, is_tsumogiri, oya_player):\n",
        "    tile_type_code = physical_code // 4\n",
        "    t_type, t_num = _parse_tile_type(tile_type_code)\n",
        "    features = []\n",
        "\n",
        "    public_tiles = []\n",
        "    public_tiles.extend(game_state['hands'][player_index])\n",
        "    for i in range(4):\n",
        "        public_tiles.extend(game_state['discards'][i])\n",
        "        for meld in game_state['open_hands'][i]:\n",
        "            public_tiles.extend(meld)\n",
        "\n",
        "    features.append(t_type)\n",
        "    features.append(t_num)\n",
        "    features.append(public_tiles.count(tile_type_code))\n",
        "\n",
        "    dora_val = 0\n",
        "    dora_indicators = game_state['dora_indicators']\n",
        "    dora_val += count_dora(tile_type_code, dora_indicators)\n",
        "    if physical_code in [16, 52, 88]: dora_val += 1\n",
        "    features.append(dora_val)\n",
        "\n",
        "    features.append(len(game_state['history']) // 4)\n",
        "    features.append(1 if is_tsumogiri else 0)\n",
        "\n",
        "    other_reach_indices = [i for i in range(4) if i != player_index and game_state['reach'][i]]\n",
        "    features.append(len(other_reach_indices))\n",
        "\n",
        "    risk_score = 0\n",
        "    if len(other_reach_indices) > 0:\n",
        "        for r_idx in other_reach_indices:\n",
        "            if tile_type_code in game_state['discards'][r_idx]: continue\n",
        "            this_risk = 0\n",
        "            if check_suji_fair(t_type, t_num, game_state['discards'][r_idx]) == 0:\n",
        "                this_risk += 50\n",
        "                if 4 <= t_num <= 6: this_risk += 20\n",
        "                kabe = get_kabe_fair(t_type, t_num, public_tiles)\n",
        "                if kabe < 1: this_risk += 30\n",
        "            else:\n",
        "                this_risk += 15\n",
        "            if this_risk > risk_score: risk_score = this_risk\n",
        "    features.append(risk_score)\n",
        "\n",
        "    live_degree = 0\n",
        "    visible_n = public_tiles.count(tile_type_code)\n",
        "    if visible_n == 0:\n",
        "        live_degree = 2\n",
        "        if t_type == 3: live_degree = 3\n",
        "    elif visible_n == 1:\n",
        "        live_degree = 1\n",
        "    features.append(live_degree)\n",
        "\n",
        "    features.append(get_kabe_fair(t_type, t_num, public_tiles))\n",
        "\n",
        "    max_meld_threat = 0\n",
        "    for i in range(4):\n",
        "        if i != player_index:\n",
        "            th = get_meld_threat_level(i, game_state, dora_indicators)\n",
        "            if th > max_meld_threat: max_meld_threat = th\n",
        "    features.append(max_meld_threat)\n",
        "\n",
        "    my_yakuman = get_my_yakuman_potential(game_state['hands'][player_index])\n",
        "    features.append(my_yakuman)\n",
        "\n",
        "    return features\n",
        "\n",
        "feature_names = [\n",
        "    '1.種類', '2.数字', '3.見え枚', '4.ドラ数',\n",
        "    '5.巡目', '6.ツモ切', '7.他家立直数',\n",
        "    '8.対リーチ危険度', '9.生牌度', '10.カベ強度',\n",
        "    '11.敵副露脅威度', '12.自手牌価値'\n",
        "]\n",
        "\n",
        "# =============================================================================\n",
        "# 4. XML解析 (バッチ保存)\n",
        "# =============================================================================\n",
        "def get_clean_tag(tag_str):\n",
        "    if '}' in tag_str:\n",
        "        return tag_str.split('}', 1)[1]\n",
        "    return tag_str\n",
        "\n",
        "print(\"\\n★ XML解析開始...\")\n",
        "xml_files = glob.glob(os.path.join(LOG_DIR, '**/*.xml'), recursive=True)\n",
        "print(f\"発見ファイル数: {len(xml_files)}\")\n",
        "\n",
        "if len(xml_files) == 0:\n",
        "    print(\"エラー: XMLファイルが見つかりません。\")\n",
        "else:\n",
        "    current_batch_data = []\n",
        "    BATCH_SIZE_FILES = 2000\n",
        "    batch_count = 0\n",
        "\n",
        "    draw_ptn = re.compile(r'^[TUVW]\\d+$')\n",
        "    discard_ptn = re.compile(r'^[DEFG]\\d+$')\n",
        "\n",
        "    count_suuhai_pos = 0\n",
        "    count_jihai_pos = 0\n",
        "\n",
        "    # ★目標値を変更\n",
        "    TARGET_SUUHAI = 200000\n",
        "    TARGET_JIHAI = 20000\n",
        "\n",
        "    print(f\"目標: 数牌正例 {TARGET_SUUHAI}件, 字牌正例 {TARGET_JIHAI}件\")\n",
        "\n",
        "    for i_file, file_path in enumerate(xml_files):\n",
        "\n",
        "        # バッチ保存\n",
        "        if i_file > 0 and i_file % BATCH_SIZE_FILES == 0:\n",
        "            if current_batch_data:\n",
        "                batch_df = pd.DataFrame(current_batch_data)\n",
        "                save_path = os.path.join(BATCH_SAVE_DIR, f'batch_{batch_count}.pkl')\n",
        "                batch_df.to_pickle(save_path)\n",
        "                batch_count += 1\n",
        "                del batch_df, current_batch_data\n",
        "                current_batch_data = []\n",
        "                gc.collect()\n",
        "                print(f\" [Batch] {i_file}完了 -> batch_{batch_count-1}.pkl (正例: 数牌{count_suuhai_pos}/字牌{count_jihai_pos})\")\n",
        "\n",
        "            if count_suuhai_pos >= TARGET_SUUHAI and count_jihai_pos >= TARGET_JIHAI:\n",
        "                print(\"★目標達成。解析終了。\")\n",
        "                break\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                try:\n",
        "                    tree = ET.parse(f)\n",
        "                    root = tree.getroot()\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            game_state = {\n",
        "                'hands': [[],[],[],[]], 'discards': [[],[],[],[]], 'open_hands': [[],[],[],[]],\n",
        "                'reach': [False]*4, 'history': [], 'dora_indicators': []\n",
        "            }\n",
        "            last_draw = [-1]*4\n",
        "            last_discard_event = None\n",
        "            oya_player = 0\n",
        "\n",
        "            for tag in root:\n",
        "                tag_name = get_clean_tag(tag.tag)\n",
        "\n",
        "                if tag_name == 'INIT':\n",
        "                    if last_discard_event:\n",
        "                        current_batch_data.append(last_discard_event)\n",
        "                        if last_discard_event['y'] == 1:\n",
        "                            if last_discard_event['X'][0] == 3: count_jihai_pos += 1\n",
        "                            else: count_suuhai_pos += 1\n",
        "\n",
        "                    oya_player = int(tag.attrib.get('oya', 0))\n",
        "                    seed = tag.attrib.get('seed', '0,0,0,0,0,0').split(',')\n",
        "                    dora_ind = int(seed[5]) // 4 if len(seed) > 5 else 0\n",
        "\n",
        "                    game_state = {\n",
        "                        'hands': [[],[],[],[]], 'discards': [[],[],[],[]], 'open_hands': [[],[],[],[]],\n",
        "                        'reach': [False]*4, 'history': [], 'dora_indicators': [dora_ind]\n",
        "                    }\n",
        "                    last_discard_event = None\n",
        "                    for p in range(4):\n",
        "                        h = tag.attrib.get(f'hai{p}')\n",
        "                        if h: game_state['hands'][p] = [int(s)//4 for s in h.split(',')]\n",
        "\n",
        "                if draw_ptn.match(tag_name):\n",
        "                    p = {'T':0,'U':1,'V':2,'W':3}[tag_name[0]]\n",
        "                    phys = int(tag_name[1:])\n",
        "                    last_draw[p] = phys\n",
        "                    game_state['hands'][p].append(phys // 4)\n",
        "\n",
        "                if discard_ptn.match(tag_name):\n",
        "                    p = {'D':0,'E':1,'F':2,'G':3}[tag_name[0]]\n",
        "                    phys = int(tag_name[1:])\n",
        "\n",
        "                    if last_discard_event:\n",
        "                        current_batch_data.append(last_discard_event)\n",
        "                        if last_discard_event['y'] == 1:\n",
        "                            if last_discard_event['X'][0] == 3: count_jihai_pos += 1\n",
        "                            else: count_suuhai_pos += 1\n",
        "\n",
        "                    is_tsumogiri = (phys == last_draw[p])\n",
        "                    feats = calculate_features_fair(game_state, p, phys, is_tsumogiri, oya_player)\n",
        "                    last_discard_event = {'X': feats, 'y': 0}\n",
        "\n",
        "                    t_type = phys // 4\n",
        "                    if t_type in game_state['hands'][p]: game_state['hands'][p].remove(t_type)\n",
        "                    game_state['discards'][p].append(t_type)\n",
        "                    game_state['history'].append((p, t_type))\n",
        "\n",
        "                if tag_name == 'N':\n",
        "                    who = int(tag.attrib.get('who'))\n",
        "                    m = int(tag.attrib.get('m'))\n",
        "                    called_tile_id = parse_called_tile(m)\n",
        "                    c_type = called_tile_id // 4\n",
        "                    removed = 0\n",
        "                    temp = game_state['hands'][who][:]\n",
        "                    for h in temp:\n",
        "                        if h == c_type and removed < 2:\n",
        "                            game_state['hands'][who].remove(h)\n",
        "                            removed += 1\n",
        "                    game_state['open_hands'][who].append([called_tile_id]*3)\n",
        "\n",
        "                    if last_discard_event:\n",
        "                        current_batch_data.append(last_discard_event)\n",
        "                        if last_discard_event['y'] == 1:\n",
        "                            if last_discard_event['X'][0] == 3: count_jihai_pos += 1\n",
        "                            else: count_suuhai_pos += 1\n",
        "                        last_discard_event = None\n",
        "\n",
        "                if tag_name == 'REACH' and tag.attrib.get('step') == '1':\n",
        "                    who_reach = int(tag.attrib.get('who'))\n",
        "                    game_state['reach'][who_reach] = True\n",
        "\n",
        "                if tag_name == 'DORA':\n",
        "                    hai = int(tag.attrib.get('hai'))\n",
        "                    game_state['dora_indicators'].append(hai // 4)\n",
        "\n",
        "                if tag_name == 'AGARI':\n",
        "                    loser = tag.attrib.get('fromWho')\n",
        "                    if loser and int(tag.attrib.get('who')) != int(loser):\n",
        "                        if last_discard_event:\n",
        "                            last_discard_event['y'] = 1\n",
        "                            current_batch_data.append(last_discard_event)\n",
        "                            if last_discard_event['X'][0] == 3: count_jihai_pos += 1\n",
        "                            else: count_suuhai_pos += 1\n",
        "                            last_discard_event = None\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    if current_batch_data:\n",
        "        batch_df = pd.DataFrame(current_batch_data)\n",
        "        save_path = os.path.join(BATCH_SAVE_DIR, f'batch_{batch_count}.pkl')\n",
        "        batch_df.to_pickle(save_path)\n",
        "        print(f\" [Batch] 最終保存完了\")\n",
        "        del batch_df, current_batch_data\n",
        "        gc.collect()\n",
        "\n",
        "# =============================================================================\n",
        "# 5. ★アンダーサンプリング (数牌20万ペア+字牌2万ペア)\n",
        "# =============================================================================\n",
        "print(\"\\n--- ステップ2: バッチ統合とサンプリング ---\")\n",
        "batch_files = glob.glob(os.path.join(BATCH_SAVE_DIR, '*.pkl'))\n",
        "\n",
        "if len(batch_files) == 0:\n",
        "    print(\"エラー: バッチファイルがありません。\")\n",
        "else:\n",
        "    all_suuhai_pos = []\n",
        "    all_suuhai_neg = []\n",
        "    all_jihai_pos = []\n",
        "    all_jihai_neg = []\n",
        "\n",
        "    # ★サンプリング目標\n",
        "    TARGET_SUUHAI_EACH = 200000\n",
        "    TARGET_JIHAI_EACH = 20000\n",
        "\n",
        "    print(f\"バッチ処理中...\")\n",
        "    for b_file in batch_files:\n",
        "        try:\n",
        "            df_b = pd.read_pickle(b_file)\n",
        "            if len(df_b) == 0: continue\n",
        "\n",
        "            X_temp = pd.DataFrame(df_b['X'].tolist(), columns=feature_names)\n",
        "            df_full = pd.concat([df_b['y'], X_temp], axis=1)\n",
        "            df_full['is_jihai'] = df_full['1.種類'] == 3\n",
        "\n",
        "            all_suuhai_pos.append(df_full[(df_full['is_jihai'] == False) & (df_full['y'] == 1)])\n",
        "            all_jihai_pos.append(df_full[(df_full['is_jihai'] == True) & (df_full['y'] == 1)])\n",
        "\n",
        "            # 負例は多すぎるので間引く(20%抽出)\n",
        "            s_n = df_full[(df_full['is_jihai'] == False) & (df_full['y'] == 0)]\n",
        "            j_n = df_full[(df_full['is_jihai'] == True) & (df_full['y'] == 0)]\n",
        "            if len(s_n) > 0: all_suuhai_neg.append(s_n.sample(frac=0.2))\n",
        "            if len(j_n) > 0: all_jihai_neg.append(j_n.sample(frac=0.2))\n",
        "\n",
        "            del df_b, df_full, X_temp\n",
        "            gc.collect()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    print(\"統合中...\")\n",
        "    df_suuhai_pos = pd.concat(all_suuhai_pos)\n",
        "    df_suuhai_neg = pd.concat(all_suuhai_neg)\n",
        "    df_jihai_pos = pd.concat(all_jihai_pos)\n",
        "    df_jihai_neg = pd.concat(all_jihai_neg)\n",
        "\n",
        "    print(f\"集計(正例): 数牌{len(df_suuhai_pos)}, 字牌{len(df_jihai_pos)}\")\n",
        "\n",
        "    n_sp = min(len(df_suuhai_pos), TARGET_SUUHAI_EACH)\n",
        "    n_sn = TARGET_SUUHAI_EACH\n",
        "    if len(df_suuhai_neg) < n_sn: n_sn = len(df_suuhai_neg)\n",
        "\n",
        "    n_jp = min(len(df_jihai_pos), TARGET_JIHAI_EACH)\n",
        "    n_jn = TARGET_JIHAI_EACH\n",
        "    if len(df_jihai_neg) < n_jn: n_jn = len(df_jihai_neg)\n",
        "\n",
        "    final_df = pd.concat([\n",
        "        df_suuhai_pos.sample(n=n_sp, random_state=42),\n",
        "        df_suuhai_neg.sample(n=n_sn, random_state=42),\n",
        "        df_jihai_pos.sample(n=n_jp, random_state=42),\n",
        "        df_jihai_neg.sample(n=n_jn, random_state=42)\n",
        "    ]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    print(f\"\\n★最終データセット: {len(final_df)}件\")\n",
        "    print(final_df['y'].value_counts())\n",
        "    final_df.to_pickle(DATASET_PKL)\n",
        "\n",
        "    # =============================================================================\n",
        "    # 6. Optuna & 学習\n",
        "    # =============================================================================\n",
        "    X = final_df[feature_names]\n",
        "    y = final_df['y']\n",
        "    cat_features = ['1.種類']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(\"\\n--- ステップ3: Optunaによる探索 ---\")\n",
        "    def objective(trial):\n",
        "        param = {\n",
        "            'objective': 'binary',\n",
        "            'metric': 'binary_logloss',\n",
        "            'verbosity': -1,\n",
        "            'boosting_type': 'gbdt',\n",
        "            'n_estimators': 300,\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "            'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
        "            'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
        "            'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
        "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        }\n",
        "        model = lgb.LGBMClassifier(**param)\n",
        "        model.fit(X_train, y_train, categorical_feature=cat_features)\n",
        "        preds = model.predict(X_test)\n",
        "        return accuracy_score(y_test, preds)\n",
        "\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=15)\n",
        "\n",
        "    print(f\"★ Best Params: {study.best_params}\")\n",
        "\n",
        "    print(\"\\n--- 最終学習 ---\")\n",
        "    best_params = study.best_params\n",
        "    final_model = lgb.LGBMClassifier(\n",
        "        objective='binary', metric='binary_logloss', n_estimators=5000,\n",
        "        learning_rate=best_params.get('learning_rate', 0.05),\n",
        "        num_leaves=best_params.get('num_leaves', 31),\n",
        "        max_depth=best_params.get('max_depth', -1),\n",
        "        min_child_samples=best_params.get('min_child_samples', 20),\n",
        "        subsample=best_params.get('subsample', 0.8),\n",
        "        colsample_bytree=best_params.get('colsample_bytree', 0.8)\n",
        "    )\n",
        "    final_model.fit(\n",
        "        X_train, y_train, eval_set=[(X_test, y_test)],\n",
        "        eval_metric='auc', callbacks=[lgb.early_stopping(100)],\n",
        "        categorical_feature=cat_features\n",
        "    )\n",
        "\n",
        "    preds = final_model.predict(X_test)\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, preds):.4f}\")\n",
        "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, preds))\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(y_test, preds))\n",
        "\n",
        "    imp = pd.DataFrame({'Feature': feature_names, 'Importance': final_model.feature_importances_}).sort_values('Importance', ascending=False)\n",
        "    print(imp.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P1vR_buOn-e",
        "outputId": "a34640af-88ac-42f9-be6e-ecf2f00cb6ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna) (6.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.45)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "\n",
            "--- ステップ1: 環境設定とデータの準備 ---\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "既に解凍済みデータがあるため、それを使用します。\n",
            "\n",
            "★ XML解析開始...\n",
            "発見ファイル数: 80000\n",
            "目標: 数牌正例 200000件, 字牌正例 20000件\n",
            " [Batch] 2000完了 -> batch_0.pkl (正例: 数牌9811/字牌788)\n",
            " [Batch] 4000完了 -> batch_1.pkl (正例: 数牌19473/字牌1613)\n",
            " [Batch] 6000完了 -> batch_2.pkl (正例: 数牌29190/字牌2418)\n",
            " [Batch] 8000完了 -> batch_3.pkl (正例: 数牌38978/字牌3166)\n",
            " [Batch] 10000完了 -> batch_4.pkl (正例: 数牌48756/字牌4003)\n",
            " [Batch] 12000完了 -> batch_5.pkl (正例: 数牌58519/字牌4811)\n",
            " [Batch] 14000完了 -> batch_6.pkl (正例: 数牌68267/字牌5634)\n",
            " [Batch] 16000完了 -> batch_7.pkl (正例: 数牌78001/字牌6429)\n",
            " [Batch] 18000完了 -> batch_8.pkl (正例: 数牌87887/字牌7220)\n",
            " [Batch] 20000完了 -> batch_9.pkl (正例: 数牌97671/字牌8020)\n",
            " [Batch] 22000完了 -> batch_10.pkl (正例: 数牌107306/字牌8852)\n",
            " [Batch] 24000完了 -> batch_11.pkl (正例: 数牌117182/字牌9654)\n",
            " [Batch] 26000完了 -> batch_12.pkl (正例: 数牌126987/字牌10423)\n",
            " [Batch] 28000完了 -> batch_13.pkl (正例: 数牌136618/字牌11204)\n",
            " [Batch] 30000完了 -> batch_14.pkl (正例: 数牌146266/字牌12024)\n",
            " [Batch] 32000完了 -> batch_15.pkl (正例: 数牌156016/字牌12803)\n",
            " [Batch] 34000完了 -> batch_16.pkl (正例: 数牌165694/字牌13641)\n",
            " [Batch] 36000完了 -> batch_17.pkl (正例: 数牌175442/字牌14460)\n",
            " [Batch] 38000完了 -> batch_18.pkl (正例: 数牌185217/字牌15314)\n",
            " [Batch] 40000完了 -> batch_19.pkl (正例: 数牌195006/字牌16098)\n",
            " [Batch] 42000完了 -> batch_20.pkl (正例: 数牌204662/字牌16955)\n",
            " [Batch] 44000完了 -> batch_21.pkl (正例: 数牌214396/字牌17782)\n",
            " [Batch] 46000完了 -> batch_22.pkl (正例: 数牌224043/字牌18575)\n",
            " [Batch] 48000完了 -> batch_23.pkl (正例: 数牌233681/字牌19384)\n",
            " [Batch] 50000完了 -> batch_24.pkl (正例: 数牌243252/字牌20194)\n",
            "★目標達成。解析終了。\n",
            "\n",
            "--- ステップ2: バッチ統合とサンプリング ---\n",
            "バッチ処理中...\n",
            "統合中...\n",
            "集計(正例): 数牌243252, 字牌20194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-15 04:54:47,555] A new study created in memory with name: no-name-c1534b43-86d3-4395-954b-584533801982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "★最終データセット: 440000件\n",
            "y\n",
            "1    220000\n",
            "0    220000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- ステップ3: Optunaによる探索 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-15 04:55:04,887] Trial 0 finished with value: 0.7729772727272727 and parameters: {'learning_rate': 0.03134259367768324, 'num_leaves': 39, 'max_depth': 11, 'min_child_samples': 48, 'subsample': 0.8857286475539741, 'colsample_bytree': 0.8008015882233956}. Best is trial 0 with value: 0.7729772727272727.\n",
            "[I 2026-01-15 04:55:18,496] Trial 1 finished with value: 0.7711136363636364 and parameters: {'learning_rate': 0.22063639265120677, 'num_leaves': 74, 'max_depth': 9, 'min_child_samples': 16, 'subsample': 0.6660047261235769, 'colsample_bytree': 0.9905036787184486}. Best is trial 0 with value: 0.7729772727272727.\n",
            "[I 2026-01-15 04:55:34,839] Trial 2 finished with value: 0.7705909090909091 and parameters: {'learning_rate': 0.1648949668878558, 'num_leaves': 149, 'max_depth': 12, 'min_child_samples': 30, 'subsample': 0.8933175423918653, 'colsample_bytree': 0.8143864709711878}. Best is trial 0 with value: 0.7729772727272727.\n",
            "[I 2026-01-15 04:55:49,152] Trial 3 finished with value: 0.7707272727272727 and parameters: {'learning_rate': 0.25541785740154266, 'num_leaves': 67, 'max_depth': 10, 'min_child_samples': 80, 'subsample': 0.7324337506307812, 'colsample_bytree': 0.9950461778886878}. Best is trial 0 with value: 0.7729772727272727.\n",
            "[I 2026-01-15 04:56:12,893] Trial 4 finished with value: 0.7729431818181818 and parameters: {'learning_rate': 0.025795848009027483, 'num_leaves': 82, 'max_depth': 8, 'min_child_samples': 36, 'subsample': 0.763864677984412, 'colsample_bytree': 0.7042984160358634}. Best is trial 0 with value: 0.7729772727272727.\n",
            "[I 2026-01-15 04:56:33,720] Trial 5 finished with value: 0.7729659090909091 and parameters: {'learning_rate': 0.0486245727946099, 'num_leaves': 57, 'max_depth': 9, 'min_child_samples': 42, 'subsample': 0.8279999207421967, 'colsample_bytree': 0.5083784046731258}. Best is trial 0 with value: 0.7729772727272727.\n",
            "[I 2026-01-15 04:56:48,760] Trial 6 finished with value: 0.7711363636363636 and parameters: {'learning_rate': 0.21540329820165047, 'num_leaves': 73, 'max_depth': 11, 'min_child_samples': 57, 'subsample': 0.7602789828984027, 'colsample_bytree': 0.7739287470379926}. Best is trial 0 with value: 0.7729772727272727.\n",
            "[I 2026-01-15 04:57:09,479] Trial 7 finished with value: 0.7705 and parameters: {'learning_rate': 0.2734699074057377, 'num_leaves': 108, 'max_depth': 9, 'min_child_samples': 11, 'subsample': 0.8672933501271289, 'colsample_bytree': 0.5283632482307754}. Best is trial 0 with value: 0.7729772727272727.\n",
            "[I 2026-01-15 04:57:26,821] Trial 8 finished with value: 0.770784090909091 and parameters: {'learning_rate': 0.22473840256179556, 'num_leaves': 129, 'max_depth': 15, 'min_child_samples': 44, 'subsample': 0.5879020096490734, 'colsample_bytree': 0.6392660267262852}. Best is trial 0 with value: 0.7729772727272727.\n",
            "[I 2026-01-15 04:57:46,354] Trial 9 finished with value: 0.7721931818181819 and parameters: {'learning_rate': 0.14663671720605936, 'num_leaves': 88, 'max_depth': 10, 'min_child_samples': 24, 'subsample': 0.831536511788683, 'colsample_bytree': 0.5180235641469397}. Best is trial 0 with value: 0.7729772727272727.\n",
            "[I 2026-01-15 04:57:59,254] Trial 10 finished with value: 0.7728409090909091 and parameters: {'learning_rate': 0.08673371945010462, 'num_leaves': 20, 'max_depth': 5, 'min_child_samples': 97, 'subsample': 0.9559118330503558, 'colsample_bytree': 0.8824206191821459}. Best is trial 0 with value: 0.7729772727272727.\n",
            "[I 2026-01-15 04:58:17,711] Trial 11 finished with value: 0.7716363636363637 and parameters: {'learning_rate': 0.014860769492497788, 'num_leaves': 37, 'max_depth': 13, 'min_child_samples': 70, 'subsample': 0.988803739825461, 'colsample_bytree': 0.6504845219232894}. Best is trial 0 with value: 0.7729772727272727.\n",
            "[I 2026-01-15 04:58:32,818] Trial 12 finished with value: 0.7733181818181818 and parameters: {'learning_rate': 0.0728829076732116, 'num_leaves': 47, 'max_depth': 7, 'min_child_samples': 50, 'subsample': 0.90660281854414, 'colsample_bytree': 0.870893248906875}. Best is trial 12 with value: 0.7733181818181818.\n",
            "[I 2026-01-15 04:58:47,637] Trial 13 finished with value: 0.7731818181818182 and parameters: {'learning_rate': 0.08187196360417333, 'num_leaves': 45, 'max_depth': 6, 'min_child_samples': 57, 'subsample': 0.9274960811974909, 'colsample_bytree': 0.8879738475193498}. Best is trial 12 with value: 0.7733181818181818.\n",
            "[I 2026-01-15 04:59:02,461] Trial 14 finished with value: 0.7729545454545454 and parameters: {'learning_rate': 0.09744453236437768, 'num_leaves': 45, 'max_depth': 6, 'min_child_samples': 62, 'subsample': 0.9411723400709252, 'colsample_bytree': 0.8730709974474896}. Best is trial 12 with value: 0.7733181818181818.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "★ Best Params: {'learning_rate': 0.0728829076732116, 'num_leaves': 47, 'max_depth': 7, 'min_child_samples': 50, 'subsample': 0.90660281854414, 'colsample_bytree': 0.870893248906875}\n",
            "\n",
            "--- 最終学習 ---\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[251]\tvalid_0's auc: 0.854611\tvalid_0's binary_logloss: 0.467906\n",
            "Accuracy: 0.7729\n",
            "\n",
            "Confusion Matrix:\n",
            " [[32702 11513]\n",
            " [ 8471 35314]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.74      0.77     44215\n",
            "           1       0.75      0.81      0.78     43785\n",
            "\n",
            "    accuracy                           0.77     88000\n",
            "   macro avg       0.77      0.77      0.77     88000\n",
            "weighted avg       0.77      0.77      0.77     88000\n",
            "\n",
            "  Feature  Importance\n",
            "     5.巡目        1906\n",
            "     2.数字        1805\n",
            "    3.見え枚        1413\n",
            " 12.自手牌価値        1275\n",
            "  10.カベ強度        1144\n",
            "8.対リーチ危険度        1044\n",
            "     1.種類         851\n",
            "    6.ツモ切         666\n",
            "  7.他家立直数         652\n",
            "    4.ドラ数         292\n",
            "11.敵副露脅威度         285\n",
            "    9.生牌度         183\n"
          ]
        }
      ]
    }
  ]
}